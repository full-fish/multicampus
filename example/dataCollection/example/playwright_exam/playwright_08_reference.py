import asyncio
import json
import aiomysql  # ë¹„ë™ê¸° MySQL ë¼ì´ë¸ŒëŸ¬ë¦¬
from playwright.async_api import async_playwright, TimeoutError, Error

# ğŸ“˜ ëŒ€ìƒ URL ëª©ë¡
URLS = [
    "https://quotes.toscrape.com/page/1/",
    "https://quotes.toscrape.com/page/2/",
    "https://quotes.toscrape.com/page/3/",
]

# MySQL ì—°ê²° ì •ë³´
DB_CONFIG = {
    "host": "localhost",
    "port": 3306,
    "user": "root",  # â† ë³¸ì¸ MySQL ê³„ì •
    "password": "1234",  # â† ë³¸ì¸ ë¹„ë°€ë²ˆí˜¸
    "db": "testdb",  # ë°ì´í„°ë² ì´ìŠ¤ëª…
}


# âš™ï¸ 1ï¸âƒ£ í…Œì´ë¸” ìƒì„± í•¨ìˆ˜
async def init_db():
    pool = None
    try:
        async with aiomysql.connect(**DB_CONFIG) as conn:
            async with conn.cursor() as cur:
                # ë°ì´í„°ë² ì´ìŠ¤ ì—†ìœ¼ë©´ ìƒì„±
                # await cur.execute("CREATE DATABASE IF NOT EXISTS testdb")
                # await cur.execute("USE testdb")

                # í…Œì´ë¸” ìƒì„±
                create_table_sql = """
                CREATE TABLE IF NOT EXISTS quote_scrap (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    page_num INT,
                    author VARCHAR(255),
                    text TEXT
                );
                """
                await cur.execute(create_table_sql)
                await conn.commit()

        # DB_CONFIG['db']= "scrape"
        # ë¹„ë™ê¸° MySQL ì—°ê²° í’€ ìƒì„±
        pool = await aiomysql.create_pool(**DB_CONFIG, minsize=1, maxsize=5)
    except aiomysql.Error as e:
        print("ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ :", e)
    except Exception as e:
        print("ë°ì´í„°ë² ì´ìŠ¤ ì¼ë°˜ì˜¤ë¥˜eee :", e)
    finally:
        return pool


# 2ï¸âƒ£ í˜ì´ì§€ í¬ë¡¤ë§ í•¨ìˆ˜
async def scrape_page(page, url, pool):
    try:
        await page.goto(url)
        quotes = await page.query_selector_all(".quote")
        page_num = int(url.split("/")[-2])

        async with pool.acquire() as conn:
            async with conn.cursor() as cur:
                for q in quotes:
                    text_el = await q.query_selector("span.text")
                    author_el = await q.query_selector("small.author")
                    text = await text_el.inner_text()
                    author = await author_el.inner_text()

                    # MySQLì— ì‚½ì…
                    insert_sql = """
                        INSERT INTO quote_scrap (page_num, author, text)
                        VALUES (%s, %s, %s);
                    """
                    await cur.execute(insert_sql, (page_num, author, text))
                await conn.commit()

        print(f"âœ… {url} ì €ì¥ ì™„ë£Œ ({len(quotes)}ê°œ)")

    except TimeoutError:
        print("ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ (TimeoutError)")
    except Error as e:
        print("Playwright ê´€ë ¨ ì˜¤ë¥˜:", e)
    except aiomysql.Error as e:
        print("ë°ì´í„° ë² ì´ìŠ¤ ì˜¤ë¥˜:", e)
    except Exception as e:
        print("ì¼ë°˜ Python ì˜¤ë¥˜:", e)


# âš™ï¸ 3ï¸âƒ£ ë©”ì¸ í•¨ìˆ˜
async def main():
    # DB ì´ˆê¸°í™”
    pool = await init_db()
    if not pool:
        return

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()

            # ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
            tasks = []
            for url in URLS:
                page = await context.new_page()
                tasks.append(scrape_page(page, url, pool))

            await asyncio.gather(*tasks)

            await context.close()
            await browser.close()
            pool.close()
            await pool.wait_closed()

        print("ğŸ‰ ëª¨ë“  ë°ì´í„°ê°€ MySQLì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except TimeoutError:
        print("ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ (TimeoutError)")
    except Error as e:
        print("Playwright ê´€ë ¨ ì˜¤ë¥˜:", e)
    except Exception as e:
        print("ì¼ë°˜ Python ì˜¤ë¥˜:", e)


# 4ï¸âƒ£ ì‹¤í–‰
if __name__ == "__main__":
    asyncio.run(main())
